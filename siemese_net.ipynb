{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "03d0dd6baf411efbb62a6aef0b43dc846668d2ca"
   },
   "source": [
    "## Updated TL;DR\n",
    "\n",
    "I am just using the pretrained weights from  [@martinpiotte](https://kaggle.com/martinpiotte). Thanks to **@suicaokhoailang** for creating the updated kernel. I think the important steps to improve to 0.9 are:\n",
    "- Get rid of `lapjv` dependency. It really slows down training/trying different ideas.\n",
    "- Load images as RGB (and retrain). I can't find where, but the current first place wrote that it helps by ~0.1.\n",
    "\n",
    "### Interesting:\n",
    "- The `mpiotte-bootstrap-model` only scored `0.697`. Though, it was better on the playgroud competition.\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "I tried to refactor [@martinpiotte](https://kaggle.com/martinpiotte)'s original kernel [here](https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563).\n",
    "\n",
    "I changed almost nothing beside commenting out the latter 380 epochs since it can't fit into a kernel. I also generated the new bounding boxes in my kernel [here](https://www.kaggle.com/suicaokhoailang/generating-whale-bounding-boxes) and saved it as a **.csv** instead of **pickle** for readability. \n",
    "\n",
    "A few things to point out:\n",
    "\n",
    "- Training more will probably improve your score, maybe as many as 500 epochs. We only train for 20 epochs in this kernel.\n",
    "\n",
    "- You may try to improve your training time by applying this technique (thanks **Brian**): https://www.kaggle.com/c/humpback-whale-identification/discussion/74402#444476 .\n",
    "\n",
    "- Consider using a pretrained model(s), good for blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lap in ./venv/lib/python3.6/site-packages (0.4.0)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install lap\n",
    "# Read the dataset description\n",
    "import gzip\n",
    "# Read or generate p2h, a dictionary of image name to image id (picture to hash)\n",
    "import pickle\n",
    "import platform\n",
    "import random\n",
    "# Suppress annoying stderr output when importing keras.\n",
    "import sys\n",
    "from lap import lapjv\n",
    "from math import sqrt\n",
    "# Determine the size of each image\n",
    "from os.path import isfile\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image as pil_image\n",
    "from imagehash import phash\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.engine.topology import Input\n",
    "from keras.layers import Activation, Add, BatchNormalization, Concatenate, Conv2D, Dense, Flatten, GlobalMaxPooling2D, \\\n",
    "    Lambda, MaxPooling2D, Reshape\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import Sequence\n",
    "from pandas import read_csv\n",
    "from scipy.ndimage import affine_transform\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "TRAIN_DF = 'data/train.csv'\n",
    "SUB_Df = 'data/sample_submission.csv'\n",
    "TRAIN = 'data/train/'\n",
    "TEST = 'data/test/'\n",
    "P2H = '../input/metadata/p2h.pickle'\n",
    "P2SIZE = '../input/metadata/p2size.pickle'\n",
    "BB_DF = 'data/bounding_boxes.csv'\n",
    "tagged = dict([(p, w) for _, p, w in read_csv(TRAIN_DF).to_records()])\n",
    "submit = [p for _, p, _ in read_csv(SUB_Df).to_records()]\n",
    "join = list(tagged.keys()) + submit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "dd0e004efca215f2d92242a17535f0b4c6f3e323"
   },
   "outputs": [],
   "source": [
    "def expand_path(p):\n",
    "    if isfile(TRAIN + p):\n",
    "        return TRAIN + p\n",
    "    if isfile(TEST + p):\n",
    "        return TEST + p\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ee3c6a569bfe93b81bcbb97a40e3e78363c79d2"
   },
   "source": [
    "## Duplicate image identification\n",
    "\n",
    "This part was from the original kernel, seems like in the playground competition dulicated images was a real issue. I don't know the case about this one but I took one for the team and generated the results anyway. I'm such a nice chap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "aa24819e1cc9ef98db60d218cc01e56f8d4e5046"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93b6dad9c544992bd799b1eb8862801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33321), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if isfile(P2SIZE):\n",
    "    print(\"P2SIZE exists.\")\n",
    "    with open(P2SIZE, 'rb') as f:\n",
    "        p2size = pickle.load(f)\n",
    "else:\n",
    "    p2size = {}\n",
    "    for p in tqdm(join):\n",
    "        size = pil_image.open(expand_path(p)).size\n",
    "        p2size[p] = size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "cdc36543403de9841a8af37f40c0044e17f6aa69"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650a328ab85c4aadb6476a57d8b79cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33321), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730b06ac5ab440d3b5e559fe5a84e9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33317), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def match(h1, h2):\n",
    "    for p1 in h2ps[h1]:\n",
    "        for p2 in h2ps[h2]:\n",
    "            i1 = pil_image.open(expand_path(p1))\n",
    "            i2 = pil_image.open(expand_path(p2))\n",
    "            if i1.mode != i2.mode or i1.size != i2.size: return False\n",
    "            a1 = np.array(i1)\n",
    "            a1 = a1 - a1.mean()\n",
    "            a1 = a1 / sqrt((a1 ** 2).mean())\n",
    "            a2 = np.array(i2)\n",
    "            a2 = a2 - a2.mean()\n",
    "            a2 = a2 / sqrt((a2 ** 2).mean())\n",
    "            a = ((a1 - a2) ** 2).mean()\n",
    "            if a > 0.1: return False\n",
    "    return True\n",
    "\n",
    "\n",
    "if isfile(P2H):\n",
    "    print(\"P2H exists.\")\n",
    "    with open(P2H, 'rb') as f:\n",
    "        p2h = pickle.load(f)\n",
    "else:\n",
    "    # Compute phash for each image in the training and test set.\n",
    "    p2h = {}\n",
    "    for p in tqdm(join):\n",
    "        img = pil_image.open(expand_path(p))\n",
    "        h = phash(img)\n",
    "        p2h[p] = h\n",
    "\n",
    "    # Find all images associated with a given phash value.\n",
    "    h2ps = {}\n",
    "    for p, h in p2h.items():\n",
    "        if h not in h2ps: h2ps[h] = []\n",
    "        if p not in h2ps[h]: h2ps[h].append(p)\n",
    "\n",
    "    # Find all distinct phash values\n",
    "    hs = list(h2ps.keys())\n",
    "\n",
    "    # If the images are close enough, associate the two phash values (this is the slow part: n^2 algorithm)\n",
    "    h2h = {}\n",
    "    for i, h1 in enumerate(tqdm(hs)):\n",
    "        for h2 in hs[:i]:\n",
    "            if h1 - h2 <= 6 and match(h1, h2):\n",
    "                s1 = str(h1)\n",
    "                s2 = str(h2)\n",
    "                if s1 < s2: s1, s2 = s2, s1\n",
    "                h2h[s1] = s2\n",
    "\n",
    "    # Group together images with equivalent phash, and replace by string format of phash (faster and more readable)\n",
    "    for p, h in p2h.items():\n",
    "        h = str(h)\n",
    "        if h in h2h: h = h2h[h]\n",
    "        p2h[p] = h\n",
    "#     with open(P2H, 'wb') as f:\n",
    "#         pickle.dump(p2h, f)\n",
    "# For each image id, determine the list of pictures\n",
    "h2ps = {}\n",
    "for p, h in p2h.items():\n",
    "    if h not in h2ps: h2ps[h] = []\n",
    "    if p not in h2ps[h]: h2ps[h].append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "62cc064dede5463c4f553da833020bee789f0de5"
   },
   "outputs": [],
   "source": [
    "def show_whale(imgs, per_row=2):\n",
    "    n = len(imgs)\n",
    "    rows = (n + per_row - 1) // per_row\n",
    "    cols = min(per_row, n)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(24 // per_row * cols, 24 // per_row * rows))\n",
    "    for ax in axes.flatten(): ax.axis('off')\n",
    "    for i, (img, ax) in enumerate(zip(imgs, axes.flatten())): ax.imshow(img.convert('RGB'))\n",
    "        \n",
    "\n",
    "def read_raw_image(p):\n",
    "    img = pil_image.open(expand_path(p))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "87e36806009c7252803a2f49d7a071dbacfafda9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33317,\n",
       " [('d26698c3271c757c', '0000e88ab.jpg'),\n",
       "  ('ba8cc231ad489b77', '0001f9222.jpg'),\n",
       "  ('bbcad234a52d0f0b', '00029d126.jpg'),\n",
       "  ('c09ae7dc09f33a29', '00050a15a.jpg'),\n",
       "  ('d02f65ba9f74a08a', '0005c1ef8.jpg')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each images id, select the prefered image\n",
    "def prefer(ps):\n",
    "    if len(ps) == 1: return ps[0]\n",
    "    best_p = ps[0]\n",
    "    best_s = p2size[best_p]\n",
    "    for i in range(1, len(ps)):\n",
    "        p = ps[i]\n",
    "        s = p2size[p]\n",
    "        if s[0] * s[1] > best_s[0] * best_s[1]:  # Select the image with highest resolution\n",
    "            best_p = p\n",
    "            best_s = s\n",
    "    return best_p\n",
    "\n",
    "h2p = {}\n",
    "for h, ps in h2ps.items():\n",
    "    h2p[h] = prefer(ps)\n",
    "len(h2p), list(h2p.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "208f5d98270a8081f7024c52f4dad14791e48182"
   },
   "outputs": [],
   "source": [
    "# Read the bounding box data from the bounding box kernel (see reference above)\n",
    "p2bb = pd.read_csv(BB_DF).set_index(\"Image\")\n",
    "\n",
    "img_shape = (384, 384, 1)  # The image shape used by the model\n",
    "anisotropy = 2.15  # The horizontal compression ratio\n",
    "crop_margin = 0.05  # The margin added around the bounding box to compensate for bounding box inaccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "0b0629a511b86810a0106f23c5e25710242edf11"
   },
   "outputs": [],
   "source": [
    "def build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    \"\"\"\n",
    "    Build a transformation matrix with the specified characteristics.\n",
    "    \"\"\"\n",
    "    rotation = np.deg2rad(rotation)\n",
    "    shear = np.deg2rad(shear)\n",
    "    rotation_matrix = np.array(\n",
    "        [[np.cos(rotation), np.sin(rotation), 0], [-np.sin(rotation), np.cos(rotation), 0], [0, 0, 1]])\n",
    "    shift_matrix = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])\n",
    "    shear_matrix = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n",
    "    zoom_matrix = np.array([[1.0 / height_zoom, 0, 0], [0, 1.0 / width_zoom, 0], [0, 0, 1]])\n",
    "    shift_matrix = np.array([[1, 0, -height_shift], [0, 1, -width_shift], [0, 0, 1]])\n",
    "    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "a9bf0641d287366059c95fcbe0f5e2812f4bf236"
   },
   "outputs": [],
   "source": [
    "def read_cropped_image(p, augment):\n",
    "    \"\"\"\n",
    "    @param p : the name of the picture to read\n",
    "    @param augment: True/False if data augmentation should be performed\n",
    "    @return a numpy array with the transformed image\n",
    "    \"\"\"\n",
    "    # If an image id was given, convert to filename\n",
    "    if p in h2p:\n",
    "        p = h2p[p]\n",
    "    size_x, size_y = p2size[p]\n",
    "\n",
    "    # Determine the region of the original image we want to capture based on the bounding box.\n",
    "    row = p2bb.loc[p]\n",
    "    x0, y0, x1, y1 = row['x0'], row['y0'], row['x1'], row['y1']\n",
    "    dx = x1 - x0\n",
    "    dy = y1 - y0\n",
    "    x0 -= dx * crop_margin\n",
    "    x1 += dx * crop_margin + 1\n",
    "    y0 -= dy * crop_margin\n",
    "    y1 += dy * crop_margin + 1\n",
    "    if x0 < 0:\n",
    "        x0 = 0\n",
    "    if x1 > size_x:\n",
    "        x1 = size_x\n",
    "    if y0 < 0:\n",
    "        y0 = 0\n",
    "    if y1 > size_y:\n",
    "        y1 = size_y\n",
    "    dx = x1 - x0\n",
    "    dy = y1 - y0\n",
    "    if dx > dy * anisotropy:\n",
    "        dy = 0.5 * (dx / anisotropy - dy)\n",
    "        y0 -= dy\n",
    "        y1 += dy\n",
    "    else:\n",
    "        dx = 0.5 * (dy * anisotropy - dx)\n",
    "        x0 -= dx\n",
    "        x1 += dx\n",
    "\n",
    "    # Generate the transformation matrix\n",
    "    trans = np.array([[1, 0, -0.5 * img_shape[0]], [0, 1, -0.5 * img_shape[1]], [0, 0, 1]])\n",
    "    trans = np.dot(np.array([[(y1 - y0) / img_shape[0], 0, 0], [0, (x1 - x0) / img_shape[1], 0], [0, 0, 1]]), trans)\n",
    "    if augment:\n",
    "        trans = np.dot(build_transform(\n",
    "            random.uniform(-5, 5),\n",
    "            random.uniform(-5, 5),\n",
    "            random.uniform(0.8, 1.0),\n",
    "            random.uniform(0.8, 1.0),\n",
    "            random.uniform(-0.05 * (y1 - y0), 0.05 * (y1 - y0)),\n",
    "            random.uniform(-0.05 * (x1 - x0), 0.05 * (x1 - x0))\n",
    "        ), trans)\n",
    "    trans = np.dot(np.array([[1, 0, 0.5 * (y1 + y0)], [0, 1, 0.5 * (x1 + x0)], [0, 0, 1]]), trans)\n",
    "\n",
    "    # Read the image, transform to black and white and convert to numpy array\n",
    "    img = read_raw_image(p).convert('L')\n",
    "    img = img_to_array(img)\n",
    "\n",
    "    # Apply affine transformation\n",
    "    matrix = trans[:2, :2]\n",
    "    offset = trans[:2, 2]\n",
    "    img = img.reshape(img.shape[:-1])\n",
    "    img = affine_transform(img, matrix, offset, output_shape=img_shape[:-1], order=1, mode='constant',\n",
    "                           cval=np.average(img))\n",
    "    img = img.reshape(img_shape)\n",
    "\n",
    "    # Normalize to zero mean and unit variance\n",
    "    img -= np.mean(img, keepdims=True)\n",
    "    img /= np.std(img, keepdims=True) + K.epsilon()\n",
    "    return img\n",
    "\n",
    "def read_for_training(p):\n",
    "    \"\"\"\n",
    "    Read and preprocess an image with data augmentation (random transform).\n",
    "    \"\"\"\n",
    "    return read_cropped_image(p, True)\n",
    "\n",
    "\n",
    "def read_for_validation(p):\n",
    "    \"\"\"\n",
    "    Read and preprocess an image without data augmentation (use for testing).\n",
    "    \"\"\"\n",
    "    return read_cropped_image(p, False)\n",
    "\n",
    "\n",
    "p = list(tagged.keys())[312]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "725e7e521b6bf776e8a165614dba50cb597d3a1c"
   },
   "outputs": [],
   "source": [
    "def subblock(x, filter, **kwargs):\n",
    "    x = BatchNormalization()(x)\n",
    "    y = x\n",
    "    y = Conv2D(filter, (1, 1), activation='relu', **kwargs)(y)  # Reduce the number of features to 'filter'\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(filter, (3, 3), activation='relu', **kwargs)(y)  # Extend the feature field\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(K.int_shape(x)[-1], (1, 1), **kwargs)(y)  # no activation # Restore the number of original features\n",
    "    y = Add()([x, y])  # Add the bypass connection\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_model(lr, l2, activation='sigmoid'):\n",
    "    ##############\n",
    "    # BRANCH MODEL\n",
    "    ##############\n",
    "    regul = regularizers.l2(l2)\n",
    "    optim = Adam(lr=lr)\n",
    "    kwargs = {'padding': 'same', 'kernel_regularizer': regul}\n",
    "\n",
    "    inp = Input(shape=img_shape)  # 384x384x1\n",
    "    x = Conv2D(64, (9, 9), strides=2, activation='relu', **kwargs)(inp)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)  # 96x96x64\n",
    "    for _ in range(2):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', **kwargs)(x)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)  # 48x48x64\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (1, 1), activation='relu', **kwargs)(x)  # 48x48x128\n",
    "    for _ in range(4):\n",
    "        x = subblock(x, 64, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)  # 24x24x128\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (1, 1), activation='relu', **kwargs)(x)  # 24x24x256\n",
    "    for _ in range(4):\n",
    "        x = subblock(x, 64, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)  # 12x12x256\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(384, (1, 1), activation='relu', **kwargs)(x)  # 12x12x384\n",
    "    for _ in range(4):\n",
    "        x = subblock(x, 96, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)  # 6x6x384\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (1, 1), activation='relu', **kwargs)(x)  # 6x6x512\n",
    "    for _ in range(4):\n",
    "        x = subblock(x, 128, **kwargs)\n",
    "\n",
    "    x = GlobalMaxPooling2D()(x)  # 512\n",
    "    branch_model = Model(inp, x, name='branch_model')\n",
    "\n",
    "    ############\n",
    "    # HEAD MODEL\n",
    "    ############\n",
    "    mid = 32\n",
    "    xa_inp = Input(shape=branch_model.output_shape[1:])\n",
    "    xb_inp = Input(shape=branch_model.output_shape[1:])\n",
    "    x1 = Lambda(lambda x: x[0] * x[1])([xa_inp, xb_inp])\n",
    "    x2 = Lambda(lambda x: x[0] + x[1])([xa_inp, xb_inp])\n",
    "    x3 = Lambda(lambda x: K.abs(x[0] - x[1]))([xa_inp, xb_inp])\n",
    "    x4 = Lambda(lambda x: K.square(x))(x3)\n",
    "    x = Concatenate()([x1, x2, x3, x4])\n",
    "    x = Reshape((4, branch_model.output_shape[1], 1), name='reshape1')(x)\n",
    "\n",
    "    # Per feature NN with shared weight is implemented using CONV2D with appropriate stride.\n",
    "    x = Conv2D(mid, (4, 1), activation='relu', padding='valid')(x)\n",
    "    x = Reshape((branch_model.output_shape[1], mid, 1))(x)\n",
    "    x = Conv2D(1, (1, mid), activation='linear', padding='valid')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "\n",
    "    # Weighted sum implemented as a Dense layer.\n",
    "    x = Dense(1, use_bias=True, activation=activation, name='weighted-average')(x)\n",
    "    head_model = Model([xa_inp, xb_inp], x, name='head')\n",
    "\n",
    "    ########################\n",
    "    # SIAMESE NEURAL NETWORK\n",
    "    ########################\n",
    "    # Complete model is constructed by calling the branch model on each input image,\n",
    "    # and then the head model on the resulting 512-vectors.\n",
    "    img_a = Input(shape=img_shape)\n",
    "    img_b = Input(shape=img_shape)\n",
    "    xa = branch_model(img_a)\n",
    "    xb = branch_model(img_b)\n",
    "    x = head_model([xa, xb])\n",
    "    model = Model([img_a, img_b], x)\n",
    "    model.compile(optim, loss='binary_crossentropy', metrics=['binary_crossentropy', 'acc'])\n",
    "    return model, branch_model, head_model\n",
    "\n",
    "\n",
    "model, branch_model, head_model = build_model(64e-5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_uuid": "d8266e1cc94c9c56ff75352aef4194aa37574972"
   },
   "outputs": [],
   "source": [
    "h2ws = {}\n",
    "new_whale = 'new_whale'\n",
    "for p, w in tagged.items():\n",
    "    if w != new_whale:  # Use only identified whales\n",
    "        h = p2h[p]\n",
    "        if h not in h2ws: h2ws[h] = []\n",
    "        if w not in h2ws[h]: h2ws[h].append(w)\n",
    "for h, ws in h2ws.items():\n",
    "    if len(ws) > 1:\n",
    "        h2ws[h] = sorted(ws)\n",
    "\n",
    "# For each whale, find the unambiguous images ids.\n",
    "w2hs = {}\n",
    "for h, ws in h2ws.items():\n",
    "    if len(ws) == 1:  # Use only unambiguous pictures\n",
    "        w = ws[0]\n",
    "        if w not in w2hs: w2hs[w] = []\n",
    "        if h not in w2hs[w]: w2hs[w].append(h)\n",
    "for w, hs in w2hs.items():\n",
    "    if len(hs) > 1:\n",
    "        w2hs[w] = sorted(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_uuid": "f122bcb1d87721a96a18c31aa2f46aaad4037039"
   },
   "outputs": [],
   "source": [
    "train = []  # A list of training image ids\n",
    "for hs in w2hs.values():\n",
    "    if len(hs) > 1:\n",
    "        train += hs\n",
    "random.shuffle(train)\n",
    "train_set = set(train)\n",
    "\n",
    "w2ts = {}  # Associate the image ids from train to each whale id.\n",
    "for w, hs in w2hs.items():\n",
    "    for h in hs:\n",
    "        if h in train_set:\n",
    "            if w not in w2ts:\n",
    "                w2ts[w] = []\n",
    "            if h not in w2ts[w]:\n",
    "                w2ts[w].append(h)\n",
    "for w, ts in w2ts.items():\n",
    "    w2ts[w] = np.array(ts)\n",
    "\n",
    "t2i = {}  # The position in train of each training image id\n",
    "for i, t in enumerate(train):\n",
    "    t2i[t] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "aa16cfd51fdbce44271883869486c1cad32f1091"
   },
   "outputs": [],
   "source": [
    "class TrainingData(Sequence):\n",
    "    def __init__(self, score, steps=1000, batch_size=32):\n",
    "        \"\"\"\n",
    "        @param score the cost matrix for the picture matching\n",
    "        @param steps the number of epoch we are planning with this score matrix\n",
    "        \"\"\"\n",
    "        super(TrainingData, self).__init__()\n",
    "        self.score = -score  # Maximizing the score is the same as minimuzing -score.\n",
    "        self.steps = steps\n",
    "        self.batch_size = batch_size\n",
    "        for ts in w2ts.values():\n",
    "            idxs = [t2i[t] for t in ts]\n",
    "            for i in idxs:\n",
    "                for j in idxs:\n",
    "                    self.score[\n",
    "                        i, j] = 10000.0  # Set a large value for matching whales -- eliminates this potential pairing\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = self.batch_size * index\n",
    "        end = min(start + self.batch_size, len(self.match) + len(self.unmatch))\n",
    "        size = end - start\n",
    "        assert size > 0\n",
    "        a = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
    "        b = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
    "        c = np.zeros((size, 1), dtype=K.floatx())\n",
    "        j = start // 2\n",
    "        for i in range(0, size, 2):\n",
    "            a[i, :, :, :] = read_for_training(self.match[j][0])\n",
    "            b[i, :, :, :] = read_for_training(self.match[j][1])\n",
    "            c[i, 0] = 1  # This is a match\n",
    "            a[i + 1, :, :, :] = read_for_training(self.unmatch[j][0])\n",
    "            b[i + 1, :, :, :] = read_for_training(self.unmatch[j][1])\n",
    "            c[i + 1, 0] = 0  # Different whales\n",
    "            j += 1\n",
    "        return [a, b], c\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.steps <= 0: return  # Skip this on the last epoch.\n",
    "        self.steps -= 1\n",
    "        self.match = []\n",
    "        self.unmatch = []\n",
    "        _, _, x = lapjv(self.score)  # Solve the linear assignment problem\n",
    "        y = np.arange(len(x), dtype=np.int32)\n",
    "\n",
    "        # Compute a derangement for matching whales\n",
    "        for ts in w2ts.values():\n",
    "            d = ts.copy()\n",
    "            while True:\n",
    "                random.shuffle(d)\n",
    "                if not np.any(ts == d): break\n",
    "            for ab in zip(ts, d): self.match.append(ab)\n",
    "\n",
    "        # Construct unmatched whale pairs from the LAP solution.\n",
    "        for i, j in zip(x, y):\n",
    "            if i == j:\n",
    "                print(self.score)\n",
    "                print(x)\n",
    "                print(y)\n",
    "                print(i, j)\n",
    "            assert i != j\n",
    "            self.unmatch.append((train[i], train[j]))\n",
    "\n",
    "        # Force a different choice for an eventual next epoch.\n",
    "        self.score[x, y] = 10000.0\n",
    "        self.score[y, x] = 10000.0\n",
    "        random.shuffle(self.match)\n",
    "        random.shuffle(self.unmatch)\n",
    "        # print(len(self.match), len(train), len(self.unmatch), len(train))\n",
    "        assert len(self.match) == len(train) and len(self.unmatch) == len(train)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.match) + len(self.unmatch) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "\n",
    "# Test on a batch of 32 with random costs.\n",
    "score = np.random.random_sample(size=(len(train), len(train)))\n",
    "data = TrainingData(score)\n",
    "(a, b), c = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_uuid": "005d041475be5ec97ac8e1d937bea6e7253226af"
   },
   "outputs": [],
   "source": [
    "# A Keras generator to evaluate only the BRANCH MODEL\n",
    "class FeatureGen(Sequence):\n",
    "    def __init__(self, data, batch_size=64, verbose=1):\n",
    "        super(FeatureGen, self).__init__()\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        if self.verbose > 0: self.progress = tqdm(total=len(self), desc='Features')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = self.batch_size * index\n",
    "        size = min(len(self.data) - start, self.batch_size)\n",
    "        a = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
    "        for i in range(size): a[i, :, :, :] = read_for_validation(self.data[start + i])\n",
    "        if self.verbose > 0:\n",
    "            self.progress.update()\n",
    "            if self.progress.n >= len(self): self.progress.close()\n",
    "        return a\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "\n",
    "class ScoreGen(Sequence):\n",
    "    def __init__(self, x, y=None, batch_size=2048, verbose=1):\n",
    "        super(ScoreGen, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        if y is None:\n",
    "            self.y = self.x\n",
    "            self.ix, self.iy = np.triu_indices(x.shape[0], 1)\n",
    "        else:\n",
    "            self.iy, self.ix = np.indices((y.shape[0], x.shape[0]))\n",
    "            self.ix = self.ix.reshape((self.ix.size,))\n",
    "            self.iy = self.iy.reshape((self.iy.size,))\n",
    "        self.subbatch = (len(self.x) + self.batch_size - 1) // self.batch_size\n",
    "        if self.verbose > 0:\n",
    "            self.progress = tqdm(total=len(self), desc='Scores')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = min(start + self.batch_size, len(self.ix))\n",
    "        a = self.y[self.iy[start:end], :]\n",
    "        b = self.x[self.ix[start:end], :]\n",
    "        if self.verbose > 0:\n",
    "            self.progress.update()\n",
    "            if self.progress.n >= len(self): self.progress.close()\n",
    "        return [a, b]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.ix) + self.batch_size - 1) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "7ac5f82779a71cca2bc2cd1bac8bac385cb48a23"
   },
   "outputs": [],
   "source": [
    "def set_lr(model, lr):\n",
    "    K.set_value(model.optimizer.lr, float(lr))\n",
    "\n",
    "\n",
    "def get_lr(model):\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "\n",
    "def score_reshape(score, x, y=None):\n",
    "    \"\"\"\n",
    "    Tranformed the packed matrix 'score' into a square matrix.\n",
    "    @param score the packed matrix\n",
    "    @param x the first image feature tensor\n",
    "    @param y the second image feature tensor if different from x\n",
    "    @result the square matrix\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        # When y is None, score is a packed upper triangular matrix.\n",
    "        # Unpack, and transpose to form the symmetrical lower triangular matrix.\n",
    "        m = np.zeros((x.shape[0], x.shape[0]), dtype=K.floatx())\n",
    "        m[np.triu_indices(x.shape[0], 1)] = score.squeeze()\n",
    "        m += m.transpose()\n",
    "    else:\n",
    "        m = np.zeros((y.shape[0], x.shape[0]), dtype=K.floatx())\n",
    "        iy, ix = np.indices((y.shape[0], x.shape[0]))\n",
    "        ix = ix.reshape((ix.size,))\n",
    "        iy = iy.reshape((iy.size,))\n",
    "        m[iy, ix] = score.squeeze()\n",
    "    return m\n",
    "\n",
    "\n",
    "def compute_score(verbose=1):\n",
    "    \"\"\"\n",
    "    Compute the score matrix by scoring every pictures from the training set against every other picture O(n^2).\n",
    "    \"\"\"\n",
    "    features = branch_model.predict_generator(FeatureGen(train, verbose=verbose), max_queue_size=12, workers=6,\n",
    "                                              verbose=0)\n",
    "    score = head_model.predict_generator(ScoreGen(features, verbose=verbose), max_queue_size=12, workers=6, verbose=0)\n",
    "    score = score_reshape(score, features)\n",
    "    return features, score\n",
    "\n",
    "\n",
    "def make_steps(step, ampl):\n",
    "    \"\"\"\n",
    "    Perform training epochs\n",
    "    @param step Number of epochs to perform\n",
    "    @param ampl the K, the randomized component of the score matrix.\n",
    "    \"\"\"\n",
    "    global w2ts, t2i, steps, features, score, histories\n",
    "\n",
    "    # shuffle the training pictures\n",
    "    random.shuffle(train)\n",
    "\n",
    "    # Map whale id to the list of associated training picture hash value\n",
    "    w2ts = {}\n",
    "    for w, hs in w2hs.items():\n",
    "        for h in hs:\n",
    "            if h in train_set:\n",
    "                if w not in w2ts: w2ts[w] = []\n",
    "                if h not in w2ts[w]: w2ts[w].append(h)\n",
    "    for w, ts in w2ts.items(): w2ts[w] = np.array(ts)\n",
    "\n",
    "    # Map training picture hash value to index in 'train' array    \n",
    "    t2i = {}\n",
    "    for i, t in enumerate(train): t2i[t] = i\n",
    "\n",
    "    # Compute the match score for each picture pair\n",
    "    features, score = compute_score()\n",
    "\n",
    "    # Train the model for 'step' epochs\n",
    "    history = model.fit_generator(\n",
    "        TrainingData(score + ampl * np.random.random_sample(size=score.shape), steps=step, batch_size=32),\n",
    "        initial_epoch=steps, epochs=steps + step, max_queue_size=12, workers=6, verbose=1).history\n",
    "    steps += step\n",
    "\n",
    "    # Collect history data\n",
    "    history['epochs'] = steps\n",
    "    history['ms'] = np.mean(score)\n",
    "    history['lr'] = get_lr(model)\n",
    "    print(history['epochs'], history['lr'], history['ms'])\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "87091f9dcd5a0d11dc255d427abd42cb6ba7506e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfb0f365a5e4b9c84f1c327694ca553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Features', max=213), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "histories = []\n",
    "steps = 0\n",
    "\n",
    "if isfile('../input/piotte/mpiotte-standard.model'):\n",
    "    tmp = keras.models.load_model('../input/piotte/mpiotte-standard.model')\n",
    "    model.set_weights(tmp.get_weights())\n",
    "else:\n",
    "    # epoch -> 10\n",
    "    make_steps(10, 1000)\n",
    "    ampl = 100.0\n",
    "    for _ in range(2):\n",
    "        print('noise ampl.  = ', ampl)\n",
    "        make_steps(5, ampl)\n",
    "        ampl = max(1.0, 100 ** -0.1 * ampl)\n",
    "#     # epoch -> 150\n",
    "#     for _ in range(18): make_steps(5, 1.0)\n",
    "#     # epoch -> 200\n",
    "#     set_lr(model, 16e-5)\n",
    "#     for _ in range(10): make_steps(5, 0.5)\n",
    "#     # epoch -> 240\n",
    "#     set_lr(model, 4e-5)\n",
    "#     for _ in range(8): make_steps(5, 0.25)\n",
    "#     # epoch -> 250\n",
    "#     set_lr(model, 1e-5)\n",
    "#     for _ in range(2): make_steps(5, 0.25)\n",
    "#     # epoch -> 300\n",
    "#     weights = model.get_weights()\n",
    "#     model, branch_model, head_model = build_model(64e-5, 0.0002)\n",
    "#     model.set_weights(weights)\n",
    "#     for _ in range(10): make_steps(5, 1.0)\n",
    "#     # epoch -> 350\n",
    "#     set_lr(model, 16e-5)\n",
    "#     for _ in range(10): make_steps(5, 0.5)\n",
    "#     # epoch -> 390\n",
    "#     set_lr(model, 4e-5)\n",
    "#     for _ in range(8): make_steps(5, 0.25)\n",
    "#     # epoch -> 400\n",
    "#     set_lr(model, 1e-5)\n",
    "#     for _ in range(2): make_steps(5, 0.25)\n",
    "#     model.save('standard.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "fdeb2d88e3ea69c5452694fa7c764231c48c590a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 384, 384, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 192, 192, 64) 5248        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 96, 96, 64)   0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 96, 96, 64)   256         max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 96, 96, 64)   36928       batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 96, 96, 64)   256         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 96, 96, 64)   36928       batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 48, 48, 64)   0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 48, 48, 64)   256         max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 48, 48, 128)  8320        batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 48, 48, 128)  512         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 48, 48, 64)   8256        batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 48, 48, 64)   256         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 48, 48, 64)   36928       batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 48, 48, 64)   256         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 48, 48, 128)  8320        batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 48, 48, 128)  0           batch_normalization_58[0][0]     \n",
      "                                                                 conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 48, 48, 128)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 48, 48, 128)  512         activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 48, 48, 64)   8256        batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 48, 48, 64)   256         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 48, 48, 64)   36928       batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 48, 48, 64)   256         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 48, 48, 128)  8320        batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 48, 48, 128)  0           batch_normalization_61[0][0]     \n",
      "                                                                 conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 48, 48, 128)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 48, 48, 128)  512         activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 48, 48, 64)   8256        batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 48, 48, 64)   256         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 48, 48, 64)   36928       batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 48, 48, 64)   256         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 48, 48, 128)  8320        batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 48, 48, 128)  0           batch_normalization_64[0][0]     \n",
      "                                                                 conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 48, 48, 128)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 48, 48, 128)  512         activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 48, 48, 64)   8256        batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 48, 48, 64)   256         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 48, 48, 64)   36928       batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 48, 48, 64)   256         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 48, 48, 128)  8320        batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 48, 48, 128)  0           batch_normalization_67[0][0]     \n",
      "                                                                 conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 48, 48, 128)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 24, 24, 128)  0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 24, 24, 128)  512         max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 24, 24, 256)  33024       batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 24, 24, 256)  1024        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 24, 24, 64)   16448       batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 24, 24, 64)   256         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 24, 24, 64)   36928       batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 24, 24, 64)   256         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 24, 24, 256)  16640       batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 24, 24, 256)  0           batch_normalization_71[0][0]     \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 24, 24, 256)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 24, 24, 256)  1024        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 24, 24, 64)   16448       batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 24, 24, 64)   256         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 24, 24, 64)   36928       batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 24, 24, 64)   256         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 24, 24, 256)  16640       batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 24, 24, 256)  0           batch_normalization_74[0][0]     \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 24, 24, 256)  0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 24, 24, 256)  1024        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 24, 24, 64)   16448       batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 24, 24, 64)   256         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 24, 24, 64)   36928       batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 24, 24, 64)   256         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 24, 24, 256)  16640       batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 24, 24, 256)  0           batch_normalization_77[0][0]     \n",
      "                                                                 conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 24, 24, 256)  0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 24, 24, 256)  1024        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 24, 24, 64)   16448       batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 24, 24, 64)   256         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 24, 24, 64)   36928       batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 24, 24, 64)   256         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 24, 24, 256)  16640       batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 24, 24, 256)  0           batch_normalization_80[0][0]     \n",
      "                                                                 conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 24, 24, 256)  0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 12, 12, 256)  0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 12, 12, 256)  1024        max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 12, 12, 384)  98688       batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 12, 12, 384)  1536        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 12, 12, 96)   36960       batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 12, 12, 96)   384         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 12, 12, 96)   83040       batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 12, 12, 96)   384         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 12, 12, 384)  37248       batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 12, 12, 384)  0           batch_normalization_84[0][0]     \n",
      "                                                                 conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 12, 12, 384)  0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 12, 12, 384)  1536        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 12, 12, 96)   36960       batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 12, 12, 96)   384         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 12, 12, 96)   83040       batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 12, 12, 96)   384         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 12, 12, 384)  37248       batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 12, 12, 384)  0           batch_normalization_87[0][0]     \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 12, 12, 384)  0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 12, 12, 384)  1536        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 12, 12, 96)   36960       batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 12, 12, 96)   384         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 12, 12, 96)   83040       batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 12, 12, 96)   384         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 12, 12, 384)  37248       batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 12, 12, 384)  0           batch_normalization_90[0][0]     \n",
      "                                                                 conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 12, 12, 384)  0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 12, 12, 384)  1536        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 12, 12, 96)   36960       batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 12, 12, 96)   384         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 12, 12, 96)   83040       batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 12, 12, 96)   384         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 12, 12, 384)  37248       batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 12, 12, 384)  0           batch_normalization_93[0][0]     \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 12, 12, 384)  0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 6, 6, 384)    0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 6, 6, 384)    1536        max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 6, 6, 512)    197120      batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 6, 6, 512)    2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 6, 6, 128)    65664       batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 6, 6, 128)    512         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 6, 6, 128)    147584      batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 6, 6, 128)    512         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 6, 6, 512)    66048       batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 6, 6, 512)    0           batch_normalization_97[0][0]     \n",
      "                                                                 conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 6, 6, 512)    0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 6, 6, 512)    2048        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 6, 6, 128)    65664       batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 6, 6, 128)    512         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 6, 6, 128)    147584      batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 6, 6, 128)    512         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 6, 6, 512)    66048       batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 6, 6, 512)    0           batch_normalization_100[0][0]    \n",
      "                                                                 conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 6, 6, 512)    0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 6, 6, 512)    2048        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 6, 6, 128)    65664       batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 6, 6, 128)    512         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 6, 6, 128)    147584      batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 6, 6, 128)    512         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 6, 6, 512)    66048       batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 6, 6, 512)    0           batch_normalization_103[0][0]    \n",
      "                                                                 conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 6, 6, 512)    0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 6, 6, 512)    2048        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 6, 6, 128)    65664       batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 6, 6, 128)    512         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 6, 6, 128)    147584      batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 6, 6, 128)    512         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 6, 6, 512)    66048       batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 6, 6, 512)    0           batch_normalization_106[0][0]    \n",
      "                                                                 conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 6, 6, 512)    0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 512)          0           activation_32[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,692,096\n",
      "Trainable params: 2,674,304\n",
      "Non-trainable params: 17,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.get_layer('branch_model').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "f194458bbb728a7cc35b75b40752e98021b6493b"
   },
   "outputs": [],
   "source": [
    "def prepare_submission(threshold, filename):\n",
    "    \"\"\"\n",
    "    Generate a Kaggle submission file.\n",
    "    @param threshold the score given to 'new_whale'\n",
    "    @param filename the submission file name\n",
    "    \"\"\"\n",
    "    vtop = 0\n",
    "    vhigh = 0\n",
    "    pos = [0, 0, 0, 0, 0, 0]\n",
    "    with open(filename, 'wt', newline='\\n') as f:\n",
    "        f.write('Image,Id\\n')\n",
    "        for i, p in enumerate(tqdm(submit)):\n",
    "            t = []\n",
    "            s = set()\n",
    "            a = score[i, :]\n",
    "            for j in list(reversed(np.argsort(a))):\n",
    "                h = known[j]\n",
    "                if a[j] < threshold and new_whale not in s:\n",
    "                    pos[len(t)] += 1\n",
    "                    s.add(new_whale)\n",
    "                    t.append(new_whale)\n",
    "                    if len(t) == 5: break;\n",
    "                for w in h2ws[h]:\n",
    "                    assert w != new_whale\n",
    "                    if w not in s:\n",
    "                        if a[j] > 1.0:\n",
    "                            vtop += 1\n",
    "                        elif a[j] >= threshold:\n",
    "                            vhigh += 1\n",
    "                        s.add(w)\n",
    "                        t.append(w)\n",
    "                        if len(t) == 5: break;\n",
    "                if len(t) == 5: break;\n",
    "            if new_whale not in s: pos[5] += 1\n",
    "            assert len(t) == 5 and len(s) == 5\n",
    "            f.write(p + ',' + ' '.join(t[:5]) + '\\n')\n",
    "    return vtop, vhigh, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "624143d234c6cc5f7374a2451aab84b955cecb09"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1f67bca34c4bf68d035665a3d79a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Features', max=246, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find elements from training sets not 'new_whale'\n",
    "tic = time.time()\n",
    "h2ws = {}\n",
    "for p, w in tagged.items():\n",
    "    if w != new_whale:  # Use only identified whales\n",
    "        h = p2h[p]\n",
    "        if h not in h2ws: h2ws[h] = []\n",
    "        if w not in h2ws[h]: h2ws[h].append(w)\n",
    "known = sorted(list(h2ws.keys()))\n",
    "\n",
    "# Dictionary of picture indices\n",
    "h2i = {}\n",
    "for i, h in enumerate(known): h2i[h] = i\n",
    "\n",
    "# Evaluate the model.\n",
    "fknown = branch_model.predict_generator(FeatureGen(known), max_queue_size=20, workers=10, verbose=0)\n",
    "fsubmit = branch_model.predict_generator(FeatureGen(submit), max_queue_size=20, workers=10, verbose=0)\n",
    "score = head_model.predict_generator(ScoreGen(fknown, fsubmit), max_queue_size=20, workers=10, verbose=0)\n",
    "score = score_reshape(score, fknown, fsubmit)\n",
    "\n",
    "# Generate the subsmission file.\n",
    "prepare_submission(0.99, 'submission.csv')\n",
    "toc = time.time()\n",
    "print(\"Submission time: \", (toc - tic) / 60.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d438a9807bb20cd7b4c3a26abbc276f0c9306c9"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
